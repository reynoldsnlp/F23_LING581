# Vector Semantics and Embeddings

## Three big topics

* Lexical semantics
  * Distributional semantics
* Vector representations
  * Sparse (tf-idf, PPMI)
  * Dense (embeddings)
* How to use embeddings
  * "Sriracha of NLP"

## 6.3 Words and Vectors

### Term-document matrices

* How are these useful for Information Retrieval (IR), i.e. finding documents
  with particular contents?

### Term-term matrices (i.e. term-context matrices)

* How do these reflect word "meaning"?

## 6.4 Cosine similarity

* Why is dot product highest when two vectors are "similar"? (Why are dot
  products of dissimilar words low?)
* What is the relationship between the dot product and vector length?
* What range is cosine similarity found in?

## 6.5-6.6 TF-IDF and PMI

* What is the goal of TF-IDF?
* What is the goal of PMI?

Between TF-IDF and PMI, which would be better at...

* ...identifying keywords in a document?
* ...identifying collocates (words that tend to co-occur)?

## 6.8 Word2vec

* What do the dimensions in a dense word embedding mean?

